{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 1000, 6)\n",
      "(64, 1000, 6)\n",
      "GPU found!\n",
      "Using device: NVIDIA GeForce RTX 3070\n",
      "Free GPU Memory: 1702 MiB\n",
      "Used GPU Memory: 6262 MiB\n",
      "Epoch 1/100, Training Loss: 0.6944053769111633, Training Accuracy: 0.48\n",
      "Epoch 1/100, Validation Loss: 0.6930904984474182, Validation Accuracy: 0.51\n",
      "Epoch 2/100, Training Loss: 0.6941732168197632, Training Accuracy: 0.47\n",
      "Epoch 2/100, Validation Loss: 0.6930719017982483, Validation Accuracy: 0.51\n",
      "Epoch 3/100, Training Loss: 0.6937803626060486, Training Accuracy: 0.48\n",
      "Epoch 3/100, Validation Loss: 0.6930720806121826, Validation Accuracy: 0.51\n",
      "Epoch 4/100, Training Loss: 0.6939166188240051, Training Accuracy: 0.50\n",
      "Epoch 4/100, Validation Loss: 0.6930733919143677, Validation Accuracy: 0.51\n",
      "Epoch 5/100, Training Loss: 0.6938019394874573, Training Accuracy: 0.46\n",
      "Epoch 5/100, Validation Loss: 0.693162739276886, Validation Accuracy: 0.49\n",
      "Epoch 6/100, Training Loss: 0.6935872435569763, Training Accuracy: 0.44\n",
      "Epoch 6/100, Validation Loss: 0.6931548714637756, Validation Accuracy: 0.49\n",
      "Epoch 7/100, Training Loss: 0.6938005685806274, Training Accuracy: 0.50\n",
      "Epoch 7/100, Validation Loss: 0.6932578086853027, Validation Accuracy: 0.49\n",
      "Epoch 8/100, Training Loss: 0.693718671798706, Training Accuracy: 0.49\n",
      "Epoch 8/100, Validation Loss: 0.6931519508361816, Validation Accuracy: 0.49\n",
      "Epoch 9/100, Training Loss: 0.6936189532279968, Training Accuracy: 0.46\n",
      "Epoch 9/100, Validation Loss: 0.6932527422904968, Validation Accuracy: 0.49\n",
      "Epoch 10/100, Training Loss: 0.6937675476074219, Training Accuracy: 0.49\n",
      "Epoch 10/100, Validation Loss: 0.6933387517929077, Validation Accuracy: 0.49\n",
      "Epoch 11/100, Training Loss: 0.6936621069908142, Training Accuracy: 0.48\n",
      "Epoch 11/100, Validation Loss: 0.6931366920471191, Validation Accuracy: 0.51\n",
      "Epoch 12/100, Training Loss: 0.6940044164657593, Training Accuracy: 0.49\n",
      "Epoch 12/100, Validation Loss: 0.6932563781738281, Validation Accuracy: 0.49\n",
      "Epoch 13/100, Training Loss: 0.6937326192855835, Training Accuracy: 0.49\n",
      "Epoch 13/100, Validation Loss: 0.6932063102722168, Validation Accuracy: 0.49\n",
      "Epoch 14/100, Training Loss: 0.6937394738197327, Training Accuracy: 0.49\n",
      "Epoch 14/100, Validation Loss: 0.6931419372558594, Validation Accuracy: 0.51\n",
      "Epoch 15/100, Training Loss: 0.6938501596450806, Training Accuracy: 0.46\n",
      "Epoch 15/100, Validation Loss: 0.6931081414222717, Validation Accuracy: 0.51\n",
      "Epoch 16/100, Training Loss: 0.6935170292854309, Training Accuracy: 0.48\n",
      "Epoch 16/100, Validation Loss: 0.6931164860725403, Validation Accuracy: 0.51\n",
      "Epoch 17/100, Training Loss: 0.6937105059623718, Training Accuracy: 0.46\n",
      "Epoch 17/100, Validation Loss: 0.6931442618370056, Validation Accuracy: 0.51\n",
      "Epoch 18/100, Training Loss: 0.6935044527053833, Training Accuracy: 0.49\n",
      "Epoch 18/100, Validation Loss: 0.6931209564208984, Validation Accuracy: 0.51\n",
      "Epoch 19/100, Training Loss: 0.6939113140106201, Training Accuracy: 0.48\n",
      "Epoch 19/100, Validation Loss: 0.6931467652320862, Validation Accuracy: 0.51\n",
      "Epoch 20/100, Training Loss: 0.6937349438667297, Training Accuracy: 0.50\n",
      "Epoch 20/100, Validation Loss: 0.6930980682373047, Validation Accuracy: 0.51\n",
      "Epoch 21/100, Training Loss: 0.6938791871070862, Training Accuracy: 0.47\n",
      "Epoch 21/100, Validation Loss: 0.6931197643280029, Validation Accuracy: 0.51\n",
      "Epoch 22/100, Training Loss: 0.6937357187271118, Training Accuracy: 0.49\n",
      "Epoch 22/100, Validation Loss: 0.6931040287017822, Validation Accuracy: 0.51\n",
      "Epoch 23/100, Training Loss: 0.6936814785003662, Training Accuracy: 0.49\n",
      "Epoch 23/100, Validation Loss: 0.6931279301643372, Validation Accuracy: 0.51\n",
      "Epoch 24/100, Training Loss: 0.693623960018158, Training Accuracy: 0.48\n",
      "Epoch 24/100, Validation Loss: 0.6931014657020569, Validation Accuracy: 0.51\n",
      "Epoch 25/100, Training Loss: 0.6934980750083923, Training Accuracy: 0.48\n",
      "Epoch 25/100, Validation Loss: 0.6930878758430481, Validation Accuracy: 0.51\n",
      "Epoch 26/100, Training Loss: 0.6937891840934753, Training Accuracy: 0.50\n",
      "Epoch 26/100, Validation Loss: 0.6931304931640625, Validation Accuracy: 0.51\n",
      "Epoch 27/100, Training Loss: 0.6941244602203369, Training Accuracy: 0.44\n",
      "Epoch 27/100, Validation Loss: 0.6931264996528625, Validation Accuracy: 0.51\n",
      "Epoch 28/100, Training Loss: 0.6937390565872192, Training Accuracy: 0.47\n",
      "Epoch 28/100, Validation Loss: 0.6931147575378418, Validation Accuracy: 0.51\n",
      "Epoch 29/100, Training Loss: 0.6939188241958618, Training Accuracy: 0.45\n",
      "Epoch 29/100, Validation Loss: 0.6931252479553223, Validation Accuracy: 0.51\n",
      "Epoch 30/100, Training Loss: 0.6938472986221313, Training Accuracy: 0.48\n",
      "Epoch 30/100, Validation Loss: 0.6930856704711914, Validation Accuracy: 0.51\n",
      "Epoch 31/100, Training Loss: 0.693537175655365, Training Accuracy: 0.48\n",
      "Epoch 31/100, Validation Loss: 0.6931269764900208, Validation Accuracy: 0.51\n",
      "Epoch 32/100, Training Loss: 0.6939767003059387, Training Accuracy: 0.48\n",
      "Epoch 32/100, Validation Loss: 0.6930978298187256, Validation Accuracy: 0.51\n",
      "Epoch 33/100, Training Loss: 0.6937012076377869, Training Accuracy: 0.49\n",
      "Epoch 33/100, Validation Loss: 0.6931055784225464, Validation Accuracy: 0.51\n",
      "Epoch 34/100, Training Loss: 0.693661630153656, Training Accuracy: 0.49\n",
      "Epoch 34/100, Validation Loss: 0.6931183934211731, Validation Accuracy: 0.51\n",
      "Epoch 35/100, Training Loss: 0.6939027309417725, Training Accuracy: 0.46\n",
      "Epoch 35/100, Validation Loss: 0.6930919885635376, Validation Accuracy: 0.51\n",
      "Epoch 36/100, Training Loss: 0.6941806077957153, Training Accuracy: 0.49\n",
      "Epoch 36/100, Validation Loss: 0.6930892467498779, Validation Accuracy: 0.51\n",
      "Epoch 37/100, Training Loss: 0.6937825679779053, Training Accuracy: 0.49\n",
      "Epoch 37/100, Validation Loss: 0.6931980848312378, Validation Accuracy: 0.49\n",
      "Epoch 38/100, Training Loss: 0.6936076879501343, Training Accuracy: 0.48\n",
      "Epoch 38/100, Validation Loss: 0.6931318044662476, Validation Accuracy: 0.51\n",
      "Epoch 39/100, Training Loss: 0.6936178803443909, Training Accuracy: 0.49\n",
      "Epoch 39/100, Validation Loss: 0.6931495666503906, Validation Accuracy: 0.49\n",
      "Epoch 40/100, Training Loss: 0.6938658952713013, Training Accuracy: 0.49\n",
      "Epoch 40/100, Validation Loss: 0.6931917071342468, Validation Accuracy: 0.49\n",
      "Epoch 41/100, Training Loss: 0.6937032341957092, Training Accuracy: 0.50\n",
      "Epoch 41/100, Validation Loss: 0.6932055950164795, Validation Accuracy: 0.49\n",
      "Epoch 42/100, Training Loss: 0.6935806274414062, Training Accuracy: 0.46\n",
      "Epoch 42/100, Validation Loss: 0.6931509375572205, Validation Accuracy: 0.49\n",
      "Epoch 43/100, Training Loss: 0.6936787366867065, Training Accuracy: 0.49\n",
      "Epoch 43/100, Validation Loss: 0.6932676434516907, Validation Accuracy: 0.49\n",
      "Epoch 44/100, Training Loss: 0.6938048005104065, Training Accuracy: 0.46\n",
      "Epoch 44/100, Validation Loss: 0.6931344270706177, Validation Accuracy: 0.51\n",
      "Epoch 45/100, Training Loss: 0.6938846111297607, Training Accuracy: 0.48\n",
      "Epoch 45/100, Validation Loss: 0.6932252049446106, Validation Accuracy: 0.49\n",
      "Epoch 46/100, Training Loss: 0.6938502788543701, Training Accuracy: 0.50\n",
      "Epoch 46/100, Validation Loss: 0.6931006908416748, Validation Accuracy: 0.51\n",
      "Epoch 47/100, Training Loss: 0.6935571432113647, Training Accuracy: 0.50\n",
      "Epoch 47/100, Validation Loss: 0.6930923461914062, Validation Accuracy: 0.51\n",
      "Epoch 48/100, Training Loss: 0.6936189532279968, Training Accuracy: 0.47\n",
      "Epoch 48/100, Validation Loss: 0.6930814981460571, Validation Accuracy: 0.51\n",
      "Epoch 49/100, Training Loss: 0.6936712861061096, Training Accuracy: 0.48\n",
      "Epoch 49/100, Validation Loss: 0.693096399307251, Validation Accuracy: 0.51\n",
      "Epoch 50/100, Training Loss: 0.6939007639884949, Training Accuracy: 0.47\n",
      "Epoch 50/100, Validation Loss: 0.6931501626968384, Validation Accuracy: 0.49\n",
      "Epoch 51/100, Training Loss: 0.6939473748207092, Training Accuracy: 0.48\n",
      "Epoch 51/100, Validation Loss: 0.6932945847511292, Validation Accuracy: 0.49\n",
      "Epoch 52/100, Training Loss: 0.693870484828949, Training Accuracy: 0.47\n",
      "Epoch 52/100, Validation Loss: 0.6932245492935181, Validation Accuracy: 0.49\n",
      "Epoch 53/100, Training Loss: 0.6935409903526306, Training Accuracy: 0.49\n",
      "Epoch 53/100, Validation Loss: 0.6931315064430237, Validation Accuracy: 0.51\n",
      "Epoch 54/100, Training Loss: 0.6936130523681641, Training Accuracy: 0.46\n",
      "Epoch 54/100, Validation Loss: 0.6931358575820923, Validation Accuracy: 0.51\n",
      "Epoch 55/100, Training Loss: 0.6935177445411682, Training Accuracy: 0.49\n",
      "Epoch 55/100, Validation Loss: 0.6931079626083374, Validation Accuracy: 0.51\n",
      "Epoch 56/100, Training Loss: 0.6935877799987793, Training Accuracy: 0.48\n",
      "Epoch 56/100, Validation Loss: 0.6931023597717285, Validation Accuracy: 0.51\n",
      "Epoch 57/100, Training Loss: 0.6936231851577759, Training Accuracy: 0.49\n",
      "Epoch 57/100, Validation Loss: 0.6931320428848267, Validation Accuracy: 0.51\n",
      "Epoch 58/100, Training Loss: 0.6937196850776672, Training Accuracy: 0.49\n",
      "Epoch 58/100, Validation Loss: 0.6931105256080627, Validation Accuracy: 0.51\n",
      "Epoch 59/100, Training Loss: 0.6936225891113281, Training Accuracy: 0.45\n",
      "Epoch 59/100, Validation Loss: 0.6931067109107971, Validation Accuracy: 0.51\n",
      "Epoch 60/100, Training Loss: 0.6936656832695007, Training Accuracy: 0.46\n",
      "Epoch 60/100, Validation Loss: 0.6931054592132568, Validation Accuracy: 0.51\n",
      "Epoch 61/100, Training Loss: 0.6936624646186829, Training Accuracy: 0.50\n",
      "Epoch 61/100, Validation Loss: 0.6931698322296143, Validation Accuracy: 0.49\n",
      "Epoch 62/100, Training Loss: 0.6935524344444275, Training Accuracy: 0.49\n",
      "Epoch 62/100, Validation Loss: 0.6931192874908447, Validation Accuracy: 0.51\n",
      "Epoch 63/100, Training Loss: 0.693712055683136, Training Accuracy: 0.50\n",
      "Epoch 63/100, Validation Loss: 0.6931588053703308, Validation Accuracy: 0.49\n",
      "Epoch 64/100, Training Loss: 0.6937034130096436, Training Accuracy: 0.48\n",
      "Epoch 64/100, Validation Loss: 0.6931027770042419, Validation Accuracy: 0.51\n",
      "Epoch 65/100, Training Loss: 0.6935797929763794, Training Accuracy: 0.50\n",
      "Epoch 65/100, Validation Loss: 0.6930946111679077, Validation Accuracy: 0.51\n",
      "Epoch 66/100, Training Loss: 0.6936812400817871, Training Accuracy: 0.48\n",
      "Epoch 66/100, Validation Loss: 0.6931011080741882, Validation Accuracy: 0.51\n",
      "Epoch 67/100, Training Loss: 0.6938872337341309, Training Accuracy: 0.48\n",
      "Epoch 67/100, Validation Loss: 0.693328857421875, Validation Accuracy: 0.49\n",
      "Epoch 68/100, Training Loss: 0.6936807632446289, Training Accuracy: 0.48\n",
      "Epoch 68/100, Validation Loss: 0.6933103799819946, Validation Accuracy: 0.49\n",
      "Epoch 69/100, Training Loss: 0.6937257647514343, Training Accuracy: 0.50\n",
      "Epoch 69/100, Validation Loss: 0.6932210326194763, Validation Accuracy: 0.49\n",
      "Epoch 70/100, Training Loss: 0.6935814023017883, Training Accuracy: 0.48\n",
      "Epoch 70/100, Validation Loss: 0.6931730508804321, Validation Accuracy: 0.49\n",
      "Epoch 71/100, Training Loss: 0.6935397982597351, Training Accuracy: 0.48\n",
      "Epoch 71/100, Validation Loss: 0.6932054758071899, Validation Accuracy: 0.49\n",
      "Epoch 72/100, Training Loss: 0.6938357949256897, Training Accuracy: 0.48\n",
      "Epoch 72/100, Validation Loss: 0.6932232975959778, Validation Accuracy: 0.49\n",
      "Epoch 73/100, Training Loss: 0.6938076019287109, Training Accuracy: 0.49\n",
      "Epoch 73/100, Validation Loss: 0.6930901408195496, Validation Accuracy: 0.51\n",
      "Epoch 74/100, Training Loss: 0.693869948387146, Training Accuracy: 0.44\n",
      "Epoch 74/100, Validation Loss: 0.6931400299072266, Validation Accuracy: 0.51\n",
      "Epoch 75/100, Training Loss: 0.6937811374664307, Training Accuracy: 0.48\n",
      "Epoch 75/100, Validation Loss: 0.693193256855011, Validation Accuracy: 0.49\n",
      "Epoch 76/100, Training Loss: 0.6935058832168579, Training Accuracy: 0.48\n",
      "Epoch 76/100, Validation Loss: 0.6931477785110474, Validation Accuracy: 0.49\n",
      "Epoch 77/100, Training Loss: 0.6935531497001648, Training Accuracy: 0.48\n",
      "Epoch 77/100, Validation Loss: 0.6932540535926819, Validation Accuracy: 0.49\n",
      "Epoch 78/100, Training Loss: 0.6936541199684143, Training Accuracy: 0.46\n",
      "Epoch 78/100, Validation Loss: 0.6931663751602173, Validation Accuracy: 0.49\n",
      "Epoch 79/100, Training Loss: 0.693795382976532, Training Accuracy: 0.46\n",
      "Epoch 79/100, Validation Loss: 0.69309002161026, Validation Accuracy: 0.51\n",
      "Epoch 80/100, Training Loss: 0.6936739087104797, Training Accuracy: 0.47\n",
      "Epoch 80/100, Validation Loss: 0.6930794715881348, Validation Accuracy: 0.51\n",
      "Epoch 81/100, Training Loss: 0.693525493144989, Training Accuracy: 0.50\n",
      "Epoch 81/100, Validation Loss: 0.6931119561195374, Validation Accuracy: 0.51\n",
      "Epoch 82/100, Training Loss: 0.6937035918235779, Training Accuracy: 0.47\n",
      "Epoch 82/100, Validation Loss: 0.6931830644607544, Validation Accuracy: 0.49\n",
      "Epoch 83/100, Training Loss: 0.6937781572341919, Training Accuracy: 0.49\n",
      "Epoch 83/100, Validation Loss: 0.6931093335151672, Validation Accuracy: 0.51\n",
      "Epoch 84/100, Training Loss: 0.6937341094017029, Training Accuracy: 0.48\n",
      "Epoch 84/100, Validation Loss: 0.6930884718894958, Validation Accuracy: 0.51\n",
      "Epoch 85/100, Training Loss: 0.6937092542648315, Training Accuracy: 0.49\n",
      "Epoch 85/100, Validation Loss: 0.693073570728302, Validation Accuracy: 0.51\n",
      "Epoch 86/100, Training Loss: 0.6935501098632812, Training Accuracy: 0.48\n",
      "Epoch 86/100, Validation Loss: 0.6930717825889587, Validation Accuracy: 0.51\n",
      "Epoch 87/100, Training Loss: 0.6938548684120178, Training Accuracy: 0.47\n",
      "Epoch 87/100, Validation Loss: 0.6930970549583435, Validation Accuracy: 0.51\n",
      "Epoch 88/100, Training Loss: 0.6935697197914124, Training Accuracy: 0.50\n",
      "Epoch 88/100, Validation Loss: 0.6932078003883362, Validation Accuracy: 0.49\n",
      "Epoch 89/100, Training Loss: 0.693587064743042, Training Accuracy: 0.48\n",
      "Epoch 89/100, Validation Loss: 0.6931651830673218, Validation Accuracy: 0.49\n",
      "Epoch 90/100, Training Loss: 0.6937445998191833, Training Accuracy: 0.46\n",
      "Epoch 90/100, Validation Loss: 0.6931434273719788, Validation Accuracy: 0.51\n",
      "Epoch 91/100, Training Loss: 0.6936286091804504, Training Accuracy: 0.48\n",
      "Epoch 91/100, Validation Loss: 0.6931418180465698, Validation Accuracy: 0.51\n",
      "Epoch 92/100, Training Loss: 0.6936691999435425, Training Accuracy: 0.48\n",
      "Epoch 92/100, Validation Loss: 0.6930840015411377, Validation Accuracy: 0.51\n",
      "Epoch 93/100, Training Loss: 0.6936302185058594, Training Accuracy: 0.48\n",
      "Epoch 93/100, Validation Loss: 0.6931243538856506, Validation Accuracy: 0.51\n",
      "Epoch 94/100, Training Loss: 0.6936991810798645, Training Accuracy: 0.48\n",
      "Epoch 94/100, Validation Loss: 0.6930721998214722, Validation Accuracy: 0.51\n",
      "Epoch 95/100, Training Loss: 0.6936600208282471, Training Accuracy: 0.48\n",
      "Epoch 95/100, Validation Loss: 0.693149983882904, Validation Accuracy: 0.49\n",
      "Epoch 96/100, Training Loss: 0.6935128569602966, Training Accuracy: 0.49\n",
      "Epoch 96/100, Validation Loss: 0.6931321024894714, Validation Accuracy: 0.51\n",
      "Epoch 97/100, Training Loss: 0.693789541721344, Training Accuracy: 0.44\n",
      "Epoch 97/100, Validation Loss: 0.6931259632110596, Validation Accuracy: 0.51\n",
      "Epoch 98/100, Training Loss: 0.6934646368026733, Training Accuracy: 0.49\n",
      "Epoch 98/100, Validation Loss: 0.693090558052063, Validation Accuracy: 0.51\n",
      "Epoch 99/100, Training Loss: 0.6936975717544556, Training Accuracy: 0.48\n",
      "Epoch 99/100, Validation Loss: 0.6932018995285034, Validation Accuracy: 0.49\n",
      "Epoch 100/100, Training Loss: 0.6934865713119507, Training Accuracy: 0.49\n",
      "Epoch 100/100, Validation Loss: 0.6932873725891113, Validation Accuracy: 0.49\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import linen as nn\n",
    "import optax\n",
    "from main import x_train, x_val, x_test, y_train, y_val, y_test # data\n",
    "import subprocess\n",
    "from flax.training import train_state\n",
    "\n",
    "\n",
    "# Specify GPU device\n",
    "gpu_devices = jax.devices(\"gpu\")\n",
    "if not gpu_devices:\n",
    "    raise RuntimeError(\"No GPU devices found.\")\n",
    "else:\n",
    "    print(\"GPU found!\")\n",
    "device = gpu_devices[0].id\n",
    "print(f\"Using device: {jax.devices()[0].device_kind}\")\n",
    "\n",
    "def get_gpu_memory_info():\n",
    "    result = subprocess.run(['nvidia-smi', '--query-gpu=memory.free,memory.used', '--format=csv,nounits,noheader'], stdout=subprocess.PIPE)\n",
    "    output = result.stdout.decode('utf-8').strip().split('\\n')\n",
    "\n",
    "    for line in output:\n",
    "        free_memory, used_memory = map(int, line.split(','))\n",
    "        \n",
    "        print(f\"Free GPU Memory: {free_memory} MiB\")\n",
    "        print(f\"Used GPU Memory: {used_memory} MiB\")\n",
    "\n",
    "get_gpu_memory_info()\n",
    "\n",
    "\n",
    "# Define the frequency layer.\n",
    "class FreqLayer(nn.Module):\n",
    "    \"\"\"Custom frequency layer.\"\"\"\n",
    "    mean_value: int\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        \"\"\"Applies pointwise product to the input x.\"\"\"\n",
    "        # Assuming x has shape (batch_size, input_size)\n",
    "        # Initialize weights with shape (input_size,)\n",
    "        \n",
    "        w = self.param('weights', nn.initializers.normal(stddev=0.01), (x.shape[1],))\n",
    "        result = (x+self.mean_value) * w\n",
    "        return result\n",
    "\n",
    "# Define the neural network model using FLAX\n",
    "class SimpleClassifier(nn.Module):\n",
    "    \"\"\"SimpleClassifier\n",
    "    Define the neural network model using FLAX\n",
    "    \n",
    "    \"\"\"\n",
    "    num_hidden: int\n",
    "    num_outputs: int \n",
    "    mean_value: float\n",
    "\n",
    "\n",
    "    @nn.compact  # Tells Flax to look for defined submodules\n",
    "    def __call__(self, x):\n",
    "        \n",
    "        x = FreqLayer(mean_value=self.mean_value, name='freqlayer')(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = nn.Dense(features=self.num_hidden, kernel_init=nn.initializers.glorot_normal(), bias_init=nn.initializers.normal())(x)\n",
    "        #print('x shape:', x.shape)\n",
    "        x = nn.leaky_relu(x)\n",
    "        x = nn.Dropout(0.25, deterministic=True)(x)\n",
    "        x = nn.Dense(features=self.num_hidden*2, kernel_init=nn.initializers.glorot_normal(), bias_init=nn.initializers.normal())(x)\n",
    "        x = nn.leaky_relu(x)\n",
    "        x = nn.Dropout(0.15, deterministic=True)(x)\n",
    "        x = nn.Dense(features=self.num_outputs)(x)\n",
    "        #x = nn.log_softmax(x) # not necessary here...\n",
    "        return x\n",
    "\n",
    "# Define loss acc and update functions\n",
    "\n",
    "def loss(params, batch):\n",
    "    inputs, labels = batch\n",
    "    logits = model.apply(params, inputs) #model instead\n",
    "    loss = jnp.mean(optax.softmax_cross_entropy(logits=logits, labels=labels))\n",
    "    return loss\n",
    "\n",
    "\n",
    "def accuracy(params, batch):\n",
    "    inputs, targets = batch\n",
    "    logits = model.apply(params, inputs)\n",
    "    return jnp.mean(jnp.argmax(logits, -1) == jnp.argmax(targets, -1))\n",
    "\n",
    "# Define a function for updating parameters using the optimizer\n",
    "@jax.jit\n",
    "def update(params, opt_state, batch):\n",
    "    grads = jax.grad(loss)(params, batch)\n",
    "    updates, new_opt_state = optimizer.update(grads, opt_state)\n",
    "    new_params = optax.apply_updates(params, updates)\n",
    "    return new_params, new_opt_state\n",
    "\n",
    "# Define inverse time decay schedule - not using yet\n",
    "def learning_rate_decay(initial_learning_rate, decay_rate, decay_steps):\n",
    "    return initial_learning_rate / (1 + (decay_rate*decay_steps))\n",
    "\n",
    "# Training data\n",
    "train_data = x_train\n",
    "train_labels = y_train\n",
    "validation_data = x_val\n",
    "validation_labels = y_val\n",
    "\n",
    "# Initialize the model params and optimizer\n",
    "rng = jax.random.PRNGKey(device)\n",
    "rng, init_key = jax.random.split(rng)\n",
    "\n",
    "model = SimpleClassifier(num_hidden=8, num_outputs=2, mean_value=2.0)\n",
    "#print(model)\n",
    "\n",
    "params = model.init(rng, jnp.ones((1, train_data.shape[1]))) # how do I init here\n",
    "#print(params)\n",
    "\n",
    "batch_size = 10\n",
    "num_epochs = 100\n",
    "num_batches = train_data.shape[0] // batch_size\n",
    "validation_interval = 1  # Validate every N epochs\n",
    "\n",
    "initial_learning_rate = 0.01\n",
    "decay_rate = 0.09\n",
    "decay_steps = train_data.shape[0]/batch_size\n",
    "\n",
    "optimizer = optax.sgd(learning_rate=initial_learning_rate, momentum=0.6)\n",
    "opt_state = optimizer.init(params)\n",
    "\n",
    "# Lists to record loss and accuracy for each epoch\n",
    "training_loss = []\n",
    "validation_loss = []\n",
    "training_accuracy = []\n",
    "validation_accuracy = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    rng, subkey = jax.random.split(rng)\n",
    "    indices = jax.random.permutation(subkey, jnp.arange(train_data.shape[0]))\n",
    "    \n",
    "    # Lists to store loss and accuracy for each batch\n",
    "    train_batch_loss, train_batch_acc = [], []\n",
    "    valid_batch_loss, valid_batch_acc = [], []\n",
    "\n",
    "    for batch_idx in range(num_batches):\n",
    "        \n",
    "        step = epoch * decay_steps + batch_idx\n",
    "\n",
    "        batch_indices = indices[batch_idx * batch_size: (batch_idx + 1) * batch_size]\n",
    "        batch = (train_data[batch_indices], train_labels[batch_indices])\n",
    "\n",
    "        # Calculate training loss and accuracy per batch\n",
    "        train_loss = loss(params, batch)\n",
    "        train_accuracy = accuracy(params, batch)\n",
    "        train_batch_loss.append(train_loss)\n",
    "        train_batch_acc.append(train_accuracy)\n",
    "\n",
    "        params, opt_state = update(params, opt_state, batch) # calls update here\n",
    "    \n",
    "    # Loss and acc for the current epoch\n",
    "    epoch_train_loss = jnp.mean(jnp.asarray(train_batch_loss))\n",
    "    epoch_train_acc = jnp.mean(jnp.asarray(train_batch_acc))\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Training Loss: {epoch_train_loss}, Training Accuracy: {epoch_train_acc:.2f}\")\n",
    "    \n",
    "    # Validation at the end of each epoch\n",
    "    if (epoch + 1) % validation_interval == 0:\n",
    "        epoch_val_loss = loss(params, (validation_data, validation_labels))\n",
    "        epoch_val_accuracy = accuracy(params, (validation_data, validation_labels))\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Validation Loss: {epoch_val_loss}, Validation Accuracy: {epoch_val_accuracy:.2f}\")\n",
    "    \n",
    "    training_loss.append(epoch_train_loss)\n",
    "    training_accuracy.append(epoch_train_acc)\n",
    "    validation_loss.append(epoch_val_loss)\n",
    "    validation_accuracy.append(epoch_val_accuracy)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
